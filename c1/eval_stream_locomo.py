# -*- coding: UTF-8 -*-
"""
@Project ï¼šgraduate
@File    ï¼ševal_stream_locomo.py
@Desc    ï¼šã€ç¬¬ä¸€ç« ï¼šæµå¼è¯„æµ‹æ ‡å‡†è„šæœ¬ - æœ€ç»ˆå®Œæ•´ç‰ˆã€‘
          åŒ…å«ï¼š
          1. Locomo æ—¶é—´æˆ³è§£æ (Inject Real Timestamp)
          2. å®Œæ•´çš„æå–-æ ¡éªŒ-å»é‡æµç¨‹
          3. å®Œæ•´çš„ QA è¯„æµ‹ä¸ F1 è®¡ç®—é€»è¾‘
          4. å®Œæ•´çš„ JSONL ç»“æœä¿å­˜é€»è¾‘
"""

import os
import sys
import json
import re
import collections
import string
from datetime import datetime
from tqdm import tqdm

# --- è·¯å¾„é€‚é… ---
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

# å¯¼å…¥ç»„ä»¶
from c1.decoupler import SemanticDecoupler, RawInputObj
from c1.verifier import ConsistencyVerifier
from c1.deduplicator import SemanticRedundancyFilter
from general.base_memory import AgenticMemorySystem
from general.model import QwenGRPOChat

# ================= é…ç½®åŒº =================
CONFIG_PATH = os.path.join(root_dir, "config", "llm_config.yaml")
TEST_DATA_PATH = os.path.join(root_dir, "dataset", "locomo10.json")
OUTPUT_MEM_PATH = os.path.join(root_dir, "c1", "output", "locomo_extracted_atoms.jsonl")
WINDOW_SIZE = 6


# =========================================

class LocomoStreamEvaluator:
    def __init__(self):
        print(">>> [Eval] åˆå§‹åŒ–æµå¼è¯„æµ‹å¼•æ“ (GRPO LoRA Version)...")
        self.llm = QwenGRPOChat(CONFIG_PATH)
        self.memory_sys = AgenticMemorySystem()

        self.decoupler = SemanticDecoupler(self.llm)
        self.verifier = ConsistencyVerifier(self.llm)
        self.deduplicator = SemanticRedundancyFilter(self.memory_sys, self.llm)

    def reset_system(self):
        """å®Œå…¨é‡ç½®è®°å¿†ç³»ç»Ÿ"""
        self.memory_sys.clear()

    def get_all_current_memories(self):
        """
        [ä¿®æ­£ç‰ˆ] è·å–å½“å‰æ‰€æœ‰è®°å¿†å†…å®¹
        """
        try:
            if hasattr(self.memory_sys, 'memory_manager'):
                # è·å–æ‰€æœ‰ MemoryNote å¯¹è±¡
                all_notes = self.memory_sys.memory_manager.get_all_memories()
                # æå– content å­—æ®µè¿”å›
                return [note.content for note in all_notes]
            else:
                return []
        except Exception as e:
            print(f"âŒ [Read Error] è¯»å–è®°å¿†å¤±è´¥: {e}")
            return []

    def _calculate_f1(self, prediction, ground_truth):
        """è®¡ç®—å•è¯çº§ F1 Score"""

        def normalize_answer(s):
            s = str(s).lower()
            s = "".join(ch for ch in s if ch not in set(string.punctuation))
            return s.split()

        pred_tokens = normalize_answer(prediction)
        gold_tokens = normalize_answer(ground_truth)

        if not pred_tokens or not gold_tokens:
            return 1.0 if pred_tokens == gold_tokens else 0.0

        common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)
        num_same = sum(common.values())

        if num_same == 0: return 0.0

        precision = 1.0 * num_same / len(pred_tokens)
        recall = 1.0 * num_same / len(gold_tokens)
        f1 = (2 * precision * recall) / (precision + recall)
        return f1

    def _parse_locomo_timestamp(self, time_str: str) -> str:
        """
        è§£æ Locomo æ—¶é—´æ ¼å¼: "6:29 pm on 7 July, 2023" -> "2023-07-07 18:29:00"
        """
        if not time_str:
            return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        try:
            dt = datetime.strptime(time_str, "%I:%M %p on %d %B, %Y")
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except ValueError:
            return str(time_str)

    def parse_locomo_sample(self, sample):
        """
        è§£ææ ·æœ¬ï¼Œå¹¶å°† session æ—¶é—´æ³¨å…¥åˆ°æ¯ä¸€ä¸ª turn ä¸­
        """
        all_turns = []
        turn_mapping = {}

        conv_data = sample.get('conversation', {})
        # è·å–æ‰€æœ‰ session key å¹¶æ’åº
        session_keys = [k for k in conv_data.keys() if 'session' in k and 'date' not in k]
        try:
            session_keys.sort(key=lambda x: int(x.split('_')[1]))
        except:
            pass

        global_idx = 0
        for s_key in session_keys:
            try:
                s_num = s_key.split('_')[1]
            except:
                s_num = "1"

            # ğŸ”¥ æ ¸å¿ƒï¼šè·å–è¯¥ Session çš„æ—¶é—´
            date_key = f"{s_key}_date_time"
            raw_time_str = conv_data.get(date_key, "")
            formatted_time = self._parse_locomo_timestamp(raw_time_str)

            turns = conv_data[s_key]
            for t_idx, turn in enumerate(turns):
                turn_id_constructed = f"D{s_num}:{t_idx + 1}"
                turn_mapping[turn_id_constructed] = global_idx
                if 'dia_id' in turn:
                    turn_mapping[turn['dia_id']] = global_idx

                # ğŸ”¥ æ³¨å…¥æ—¶é—´æˆ³
                turn['timestamp'] = formatted_time

                all_turns.append(turn)
                global_idx += 1

        questions = sample.get('qa', [])
        q_map = {}

        for q in questions:
            evidence_raw_list = q.get('evidence', [])
            trigger_idx = -1
            if evidence_raw_list:
                max_idx = -1
                for ev_str in evidence_raw_list:
                    sub_ids = re.split(r'[;,\s]+', ev_str)
                    for sub_id in sub_ids:
                        sub_id = sub_id.strip()
                        if not sub_id: continue
                        idx = turn_mapping.get(sub_id)
                        if idx is not None and idx > max_idx:
                            max_idx = idx
                if max_idx != -1:
                    trigger_idx = max_idx

            # å¦‚æœæ²¡æ‰¾åˆ° evidenceï¼Œé»˜è®¤æŒ‚è½½åˆ°æœ€åä¸€å¥
            if trigger_idx == -1:
                trigger_idx = len(all_turns) - 1

            if trigger_idx not in q_map:
                q_map[trigger_idx] = []
            q_map[trigger_idx].append(q)

        return all_turns, q_map

    def run(self, limit=1):
        """æ‰§è¡Œå®Œæ•´è¯„æµ‹æµç¨‹"""
        if not os.path.exists(TEST_DATA_PATH):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {TEST_DATA_PATH}")
            return

        os.makedirs(os.path.dirname(OUTPUT_MEM_PATH), exist_ok=True)
        with open(OUTPUT_MEM_PATH, 'w', encoding='utf-8') as f:
            pass

        print(f"ğŸ“‚ [DEBUG] ç»“æœä¿å­˜è‡³: {OUTPUT_MEM_PATH}")

        with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)

        test_samples = data[:limit] if limit else data
        print(f"\nğŸš€ å¼€å§‹è¯„æµ‹ (Limit={len(test_samples)})...")
        print("-" * 50)

        all_f1_scores = []

        with open(OUTPUT_MEM_PATH, 'a', encoding='utf-8') as f_out:

            for idx, sample in enumerate(test_samples):
                self.reset_system()
                source_id = sample.get('source_id') or sample.get('id') or f"sample_{idx}"
                all_turns, q_map = self.parse_locomo_sample(sample)
                history_buffer = []

                print(f"\nğŸ”¶ å¤„ç†æ ·æœ¬: {source_id} (å…± {len(all_turns)} è½®å¯¹è¯)")

                # === 1. éå†å¯¹è¯æµ ===
                for i, turn in enumerate(all_turns):
                    current_text = f"[{turn['speaker']}]: {turn['text']}"
                    context_text = "\n".join(history_buffer[-WINDOW_SIZE:]) if history_buffer else ""
                    turn_timestamp = turn.get('timestamp')

                    print(f"\n--- Turn {i + 1} ---")
                    print(f"Time: {turn_timestamp}")
                    print(f"Target: {current_text}")

                    # [Step 1] æå– (ä¼ å…¥æ—¶é—´æˆ³)
                    raw_obj = RawInputObj(
                        text=current_text,
                        context=context_text,
                        timestamp=turn_timestamp
                    )
                    dirty_atoms = self.decoupler.decouple(raw_obj)

                    # [Step 2] æ ¡éªŒ
                    if dirty_atoms:
                        print(f"âœ… [Decoupler] æå–: {[a.content for a in dirty_atoms]}")
                        full_evidence = f"{context_text}\n{current_text}"
                        clean_atoms = self.verifier.verify_batch(dirty_atoms, full_evidence)

                        if clean_atoms:
                            # [Step 3] å­˜å‚¨
                            self.deduplicator.filter_and_add_batch(clean_atoms)
                            print(f"ğŸ“¥ [Memory] å…¥åº“æˆåŠŸ (å½“å‰åº“å¤§å°: {len(self.get_all_current_memories())})")
                        else:
                            print("âœ‚ï¸ [Verifier] å…¨éƒ¨æ‹¦æˆª")
                    else:
                        print("âš ï¸ [Decoupler] æå–ä¸ºç©º")

                    history_buffer.append(current_text)

                    # === 2. è§¦å‘ QA (å®Œæ•´é€»è¾‘) ===
                    if i in q_map:
                        for q_item in q_map[i]:
                            print("â“ è§¦å‘ QA æµ‹è¯•...")
                            question_text = q_item.get('question', '')
                            gold_answer = (q_item.get('answer') or q_item.get('answer_text') or q_item.get(
                                'adversarial_answer') or "")

                            if not gold_answer: continue

                            # æ£€ç´¢ç›¸å…³è®°å¿†
                            relevant_mems = self.memory_sys.find_related_memories(question_text, k=3)
                            # è¿™é‡Œ relevant_mems æ˜¯ MemoryNote å¯¹è±¡åˆ—è¡¨
                            mem_context = "\n".join([f"- {m.content}" for m in
                                                     relevant_mems]) if relevant_mems else "No relevant memory found."

                            # æ„é€  Prompt
                            qa_system = "You are a helpful assistant. Answer the question based strictly on the provided memories."
                            prompt_content = f"Memories:\n{mem_context}\n\nQuestion: {question_text}\nAnswer (briefly):"
                            messages = [{"role": "system", "content": qa_system},
                                        {"role": "user", "content": prompt_content}]

                            # è°ƒç”¨ LLM (QA ä¸éœ€è¦ JSON)
                            response_dict = self.llm.chat(messages, parse_json=False)

                            if isinstance(response_dict, dict):
                                prediction = response_dict.get("answer") or response_dict.get("content") or str(
                                    response_dict)
                            else:
                                prediction = str(response_dict)

                            # è®¡ç®— F1
                            f1 = self._calculate_f1(str(prediction), str(gold_answer))
                            all_f1_scores.append(f1)
                            print(f"   [QA Result] F1: {f1:.2f} | Pred: {prediction} | Gold: {gold_answer}")

                # === 3. ä¿å­˜ç»“æœ (å®Œæ•´ä¿å­˜) ===
                final_memories = self.get_all_current_memories()
                print(f"ğŸ æœ¬æ ·æœ¬æœ€ç»ˆè®°å¿†æ•°: {len(final_memories)}")

                record = {
                    "source_id": source_id,
                    "extracted_atom_count": len(final_memories),
                    "memory_atoms": final_memories
                }
                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                f_out.flush()

        # æ‰“å°æœ€ç»ˆå¹³å‡åˆ†
        if all_f1_scores:
            final_score = sum(all_f1_scores) / len(all_f1_scores)
            print(f"\n{'=' * 40}")
            print(f"âœ… è¯„æµ‹å®Œæˆ | Final Avg F1: {final_score:.4f}")
            print(f"{'=' * 40}")
        else:
            print("\nâš ï¸ è·‘å®Œäº†ï¼Œä½†æ²¡æœ‰è§¦å‘ QA æˆ–æ²¡æœ‰æœ‰æ•ˆåˆ†æ•°ã€‚")


if __name__ == "__main__":
    evaluator = LocomoStreamEvaluator()
    # è·‘å‰ 5 ä¸ªæ ·æœ¬
    evaluator.run(limit=5)