# -*- coding: UTF-8 -*-
"""
@Project ï¼šgraduate
@File    ï¼ševal_stream_locomo.py
@Date    ï¼š2026/1/13 20:38
@Desc    ï¼šã€æµå¼è¯„æµ‹å¼•æ“ã€‘æ¨¡æ‹ŸçœŸå® Agent çš„ "è§‚å¯Ÿ-æå–-è®°å¿†-å›ç­”" é—­ç¯ã€‚
          åŒ…å« Mock æ•°æ®æ¨¡å¼ï¼Œå¯ç›´æ¥è¿è¡ŒéªŒè¯ä»£ç é€»è¾‘ã€‚
"""

import os
import sys
import json
import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# --- è·¯å¾„é€‚é… ---
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

from c1.prompts import DecouplerPrompt

# ================= é…ç½®åŒº =================
# æ‚¨çš„åº•åº§æ¨¡å‹
BASE_MODEL_PATH = "Qwen/Qwen2.5-7B-Instruct"
# æ‚¨çš„ GRPO è®­ç»ƒç»“æœ (è®­ç»ƒå®Œåè¿™é‡Œä¼šæœ‰æ–‡ä»¶)
LORA_PATH = os.path.join(current_dir, "output", "grpo_v1")

# LoCoMo æµ‹è¯•æ•°æ®è·¯å¾„ (å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨ Mock æ•°æ®)
LOCOMO_FILE = os.path.join(root_dir, "data", "locomo_test.json")


# =========================================

class GrpoMemoryAgent:
    def __init__(self, use_lora=True):
        print(f">>> æ­£åœ¨åŠ è½½æ¨¡å‹... (LoRAå¯ç”¨: {use_lora})")
        self.tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

        # åŠ è½½åº•åº§
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        if use_lora and os.path.exists(LORA_PATH):
            print(f">>> åŠ è½½ GRPO é€‚é…å™¨: {LORA_PATH}")
            self.model = PeftModel.from_pretrained(base_model, LORA_PATH)
        else:
            print("âš ï¸ æœªæ‰¾åˆ° LoRA æƒé‡æˆ–è¢«ç¦ç”¨ï¼Œå°†ä½¿ç”¨çº¯åº•åº§æ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚")
            self.model = base_model

        self.model.eval()

        # è¿è¡Œæ—¶çŠ¶æ€
        self.memory_stream = []  # å­˜ JSON å­—ç¬¦ä¸²
        self.history_buffer = []  # æ»‘åŠ¨çª—å£ç¼“å­˜

    def observe_turn(self, user_text, agent_text):
        """
        ã€ç¬¬ä¸€ç« æ ¸å¿ƒã€‘æµå¼æå–ï¼šçœ‹ä¸€è½®ï¼Œè®°ä¸€è½®ã€‚
        """
        current_turn = f"[User]: {user_text}\n[Agent]: {agent_text}"

        # 1. æ„é€  Input (åˆ©ç”¨æ»‘åŠ¨çª—å£)
        context_str = "\n".join(self.history_buffer[-4:])
        prompt = DecouplerPrompt.build_user_input(context_str, current_turn)

        messages = [
            {"role": "system", "content": DecouplerPrompt.SYSTEM},
            {"role": "user", "content": prompt}
        ]

        # 2. æ¨ç† (æå– JSON)
        text_input = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(text_input, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            # åŸå­æå–é€šå¸¸å¾ˆçŸ­ï¼Œ200 token è¶³å¤Ÿ
            outputs = self.model.generate(**inputs, max_new_tokens=200, temperature=0.01)

        extracted_content = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

        # 3. å­˜å‚¨ (ç®€å•æ¸…æ´—)
        # å¦‚æœæ¨¡å‹è¾“å‡ºäº†æœ‰æ•ˆå†…å®¹ (ä¸åªæ˜¯ç©ºçš„ [])ï¼Œå°±å­˜å…¥è®°å¿†æµ
        if "semantic_" in extracted_content or "episodic_" in extracted_content:
            # æ‰“å°å‡ºæ¥è®©æ‚¨çœ‹çœ‹æ•ˆæœ (å®šæ€§åˆ†æ)
            print(f"  ğŸ” [æå–è®°å¿†]: {extracted_content[:100]}...")
            self.memory_stream.append(extracted_content)

        # 4. æ›´æ–°æ»‘åŠ¨çª—å£
        self.history_buffer.append(current_turn)

    def answer_question(self, question):
        """
        ã€éªŒè¯ç¯èŠ‚ã€‘åŸºäºæå–å‡ºçš„è®°å¿†å›ç­”é—®é¢˜
        """
        # ç®€å•ç­–ç•¥ï¼šæ‹¼æ¥æ‰€æœ‰è®°å¿†
        memory_context = "\n".join(self.memory_stream)

        solve_prompt = f"""
Based on the following extracted memories, answer the question briefly.

### Memories:
{memory_context}

### Question:
{question}

### Answer:
"""
        inputs = self.tokenizer(solve_prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_new_tokens=100, temperature=0.1)

        return self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)


def get_test_data():
    """è·å–æµ‹è¯•æ•°æ® (ä¼˜å…ˆè¯»å–æ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨ Mock)"""
    if os.path.exists(LOCOMO_FILE):
        print(f">>> è¯»å– LoCoMo æ–‡ä»¶: {LOCOMO_FILE}")
        with open(LOCOMO_FILE, 'r') as f:
            return json.load(f)
    else:
        print(">>> âš ï¸ ä½¿ç”¨å†…ç½® Mock æ•°æ®è¿›è¡Œæµ‹è¯• (ä»…éªŒè¯ä»£ç é€»è¾‘)")
        return [
            {
                "session_id": "mock_001",
                "history": [
                    {"user": "Hi, I am Alex. I love spicy food.", "agent": "Nice to meet you Alex!"},
                    {"user": "I recently bought a Tesla Model 3.", "agent": "Wow, nice car!"},
                    {"user": "But I hate the touch screen controls.", "agent": "Yeah, that's a common complaint."}
                ],
                "questions": [
                    {"trigger_turn": 0, "text": "What kind of food does the user like?", "answer": "Spicy food"},
                    {"trigger_turn": 2, "text": "Why does the user dislike their car?",
                     "answer": "Touch screen controls"}
                ]
            }
        ]


def run_streaming_eval():
    # 1. å‡†å¤‡ç¯å¢ƒ
    data = get_test_data()
    # å¦‚æœè®­ç»ƒè¿˜æ²¡å®Œï¼Œè¿™é‡Œè®¾ä¸º False å¯ä»¥å…ˆè·‘é€šåº•åº§é€»è¾‘
    use_lora = os.path.exists(LORA_PATH)
    agent = GrpoMemoryAgent(use_lora=use_lora)

    total_correct = 0
    total_questions = 0

    # 2. éå† Session
    for session in data:
        print(f"\n{'=' * 40}")
        print(f"ğŸ¬ Session Start: {session.get('session_id')}")

        # é‡ç½® Agent çŠ¶æ€
        agent.memory_stream = []
        agent.history_buffer = []

        turns = session['history']
        # å»ºç«‹ç´¢å¼•ï¼šç¬¬å‡ è½®è§¦å‘ä»€ä¹ˆé—®é¢˜
        q_map = {}
        for q in session['questions']:
            idx = q['trigger_turn']
            if idx not in q_map: q_map[idx] = []
            q_map[idx].append(q)

        # 3. æµå¼å¾ªç¯ (Streaming Loop)
        for i, turn in enumerate(turns):
            u_text = turn.get('user', '')
            a_text = turn.get('agent', '')

            print(f"Turn {i}: User said '{u_text[:30]}...'")

            # --- åŠ¨ä½œ: è§‚å¯Ÿå¹¶æå– ---
            agent.observe_turn(u_text, a_text)

            # --- åŠ¨ä½œ: è§¦å‘æµ‹è¯• ---
            if i in q_map:
                for q in q_map[i]:
                    print(f"\n  â“ [Question]: {q['text']}")
                    pred = agent.answer_question(q['text'])
                    print(f"  ğŸ¤– [Answer]: {pred.strip()}")
                    print(f"  âœ… [Gold]: {q['answer']}")

                    # ç®€å•æ‰“åˆ†
                    if q['answer'].lower() in pred.lower():
                        print("  Result: Correct ğŸ‰")
                        total_correct += 1
                    else:
                        print("  Result: Wrong âŒ")
                    total_questions += 1
                    print("-" * 20)

    print(f"\n Final Score: {total_correct}/{total_questions} (Accuracy: {total_correct / total_questions:.2%})")


if __name__ == "__main__":
    run_streaming_eval()