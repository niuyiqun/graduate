# -*- coding: UTF-8 -*-
"""
@Project ï¼šgraduate
@File    ï¼ševal_stream_locomo.py
@Desc    ï¼šã€ç¬¬ä¸€ç« ï¼šæµå¼è¯„æµ‹æ ‡å‡†è„šæœ¬ - æœ€ç»ˆç‰ˆã€‘
          åŠŸèƒ½å‡çº§ï¼š
          1. æ”¯æŒ GRPO å¾®è°ƒæ¨¡å‹ (QwenGRPOChat)
          2. âœ… æ–°å¢ï¼šå°†ç”Ÿæˆçš„è®°å¿†åŸå­æŒä¹…åŒ–å­˜å‚¨åˆ° JSONLï¼Œä¾› C2/C3 ä½¿ç”¨
"""

import os
import sys
import json
import re
import collections
import string
from tqdm import tqdm

# --- è·¯å¾„é€‚é… ---
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

# å¯¼å…¥ç»„ä»¶
from c1.decoupler import SemanticDecoupler, RawInputObj
from c1.verifier import ConsistencyVerifier
from c1.deduplicator import SemanticRedundancyFilter
from general.base_memory import AgenticMemorySystem
from general.model import QwenGRPOChat  # <--- ä½¿ç”¨ GRPO æ¨¡å‹

# ================= é…ç½®åŒº =================
CONFIG_PATH = os.path.join(root_dir, "config", "llm_config.yaml")
TEST_DATA_PATH = os.path.join(root_dir, "dataset", "locomo10.json")
# ç»“æœä¿å­˜è·¯å¾„ (ä¾› C2/C3 ä½¿ç”¨)
OUTPUT_MEM_PATH = os.path.join(root_dir, "c1", "output", "locomo_extracted_atoms.jsonl")
WINDOW_SIZE = 6


# =========================================

class LocomoStreamEvaluator:
    def __init__(self):
        print(">>> [Eval] åˆå§‹åŒ–æµå¼è¯„æµ‹å¼•æ“ (GRPO LoRA Version)...")
        self.llm = QwenGRPOChat(CONFIG_PATH)
        self.memory_sys = AgenticMemorySystem()

        self.decoupler = SemanticDecoupler(self.llm)
        self.verifier = ConsistencyVerifier(self.llm)
        self.deduplicator = SemanticRedundancyFilter(self.memory_sys, self.llm)

    def reset_system(self):
        """å®Œå…¨é‡ç½®è®°å¿†ç³»ç»Ÿ"""
        self.memory_sys.clear()

    def get_all_current_memories(self):
        """
        è·å–å½“å‰è®°å¿†åº“ä¸­çš„æ‰€æœ‰åŸå­å†…å®¹
        å‡è®¾ memory_sys å†…éƒ¨æœ‰ä¸€ä¸ª memories åˆ—è¡¨ï¼Œä¸”æ¯ä¸ª memory å¯¹è±¡æœ‰ content å±æ€§
        """
        # å…¼å®¹ä¸åŒçš„ memory_sys å®ç°ï¼Œå°è¯•è·å–åŸå­åˆ—è¡¨
        if hasattr(self.memory_sys, 'memories'):
            return [m.content for m in self.memory_sys.memories]
        elif hasattr(self.memory_sys, 'get_all'):
            return self.memory_sys.get_all()
        else:
            return []

    def _calculate_f1(self, prediction, ground_truth):
        """è®¡ç®—å•è¯çº§ F1 Score"""

        def normalize_answer(s):
            s = str(s).lower()
            s = "".join(ch for ch in s if ch not in set(string.punctuation))
            return s.split()

        pred_tokens = normalize_answer(prediction)
        gold_tokens = normalize_answer(ground_truth)

        if not pred_tokens or not gold_tokens:
            return 1.0 if pred_tokens == gold_tokens else 0.0

        common = collections.Counter(pred_tokens) & collections.Counter(gold_tokens)
        num_same = sum(common.values())

        if num_same == 0:
            return 0.0

        precision = 1.0 * num_same / len(pred_tokens)
        recall = 1.0 * num_same / len(gold_tokens)
        f1 = (2 * precision * recall) / (precision + recall)
        return f1

    def parse_locomo_sample(self, sample):
        """è§£æ LoCoMo æ ¼å¼"""
        all_turns = []
        turn_mapping = {}

        conv_data = sample.get('conversation', {})
        session_keys = [k for k in conv_data.keys() if 'session' in k and 'date' not in k]
        try:
            session_keys.sort(key=lambda x: int(x.split('_')[1]))
        except:
            pass

        global_idx = 0
        for s_key in session_keys:
            try:
                s_num = s_key.split('_')[1]
            except:
                s_num = "1"

            turns = conv_data[s_key]
            for t_idx, turn in enumerate(turns):
                turn_id_constructed = f"D{s_num}:{t_idx + 1}"
                turn_mapping[turn_id_constructed] = global_idx
                if 'dia_id' in turn:
                    turn_mapping[turn['dia_id']] = global_idx
                all_turns.append(turn)
                global_idx += 1

        questions = sample.get('qa', [])
        q_map = {}

        for q in questions:
            evidence_raw_list = q.get('evidence', [])
            trigger_idx = -1
            if evidence_raw_list:
                max_idx = -1
                for ev_str in evidence_raw_list:
                    sub_ids = re.split(r'[;,\s]+', ev_str)
                    for sub_id in sub_ids:
                        sub_id = sub_id.strip()
                        if not sub_id: continue
                        idx = turn_mapping.get(sub_id)
                        if idx is not None and idx > max_idx:
                            max_idx = idx
                if max_idx != -1:
                    trigger_idx = max_idx

            if trigger_idx == -1:
                trigger_idx = len(all_turns) - 1

            if trigger_idx not in q_map:
                q_map[trigger_idx] = []
            q_map[trigger_idx].append(q)

        return all_turns, q_map

    def run(self, limit=5):
        """æ‰§è¡Œè¯„æµ‹å¹¶ä¿å­˜è®°å¿†"""
        if not os.path.exists(TEST_DATA_PATH):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {TEST_DATA_PATH}")
            return

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(OUTPUT_MEM_PATH), exist_ok=True)
        # æ¸…ç©ºæ—§æ–‡ä»¶ï¼Œé‡æ–°å†™å…¥
        with open(OUTPUT_MEM_PATH, 'w', encoding='utf-8') as f:
            pass

        print(f"ğŸ“‚ è®°å¿†æå–ç»“æœå°†ä¿å­˜è‡³: {OUTPUT_MEM_PATH}")

        with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)

        test_samples = data[:limit] if limit else data
        all_f1_scores = []

        print(f"\nğŸš€ å¼€å§‹è¯„æµ‹ (GRPO Model) | æ ·æœ¬æ•°: {len(test_samples)}")
        print("-" * 50)

        # æ‰“å¼€æ–‡ä»¶å‡†å¤‡è¿½åŠ å†™å…¥ (Append Mode)
        with open(OUTPUT_MEM_PATH, 'a', encoding='utf-8') as f_out:

            for idx, sample in enumerate(tqdm(test_samples, desc="Processing Samples")):
                self.reset_system()  # æ¯ä¸ªæ ·æœ¬å¼€å§‹å‰æ¸…ç©ºè®°å¿†åº“

                # è·å–æ ·æœ¬ IDï¼Œæ–¹ä¾¿åç»­ C2/C3 å¯¹åº”
                source_id = sample.get('source_id') or sample.get('id') or f"sample_{idx}"

                all_turns, q_map = self.parse_locomo_sample(sample)
                history_buffer = []

                # === 1. éå†å¯¹è¯æµï¼Œæå–è®°å¿† ===
                for i, turn in enumerate(all_turns):
                    current_text = f"[{turn['speaker']}]: {turn['text']}"
                    context_text = "\n".join(history_buffer[-WINDOW_SIZE:]) if history_buffer else ""

                    # DEBUG: æ‰“å°
                    # print(f"Processing Turn {i}: {current_text[:50]}...")

                    try:
                        raw_obj = RawInputObj(text=current_text, context=context_text)
                        dirty_atoms = self.decoupler.decouple(raw_obj)

                        if dirty_atoms:
                            # éªŒè¯ + å­˜å…¥è®°å¿†åº“
                            full_evidence = f"{context_text}\n{current_text}"
                            clean_atoms = self.verifier.verify_batch(dirty_atoms, full_evidence)
                            if clean_atoms:
                                self.deduplicator.filter_and_add_batch(clean_atoms)
                    except Exception as e:
                        # print(f"âŒ Error: {e}")
                        pass

                    history_buffer.append(current_text)

                    # === 2. è§¦å‘ QA (è®¡ç®— F1) ===
                    if i in q_map:
                        for q_item in q_map[i]:
                            question_text = q_item.get('question', '')
                            gold_answer = (q_item.get('answer') or q_item.get('answer_text') or q_item.get(
                                'adversarial_answer') or "")
                            if not gold_answer: continue

                            relevant_mems = self.memory_sys.find_related_memories(question_text, k=3)
                            mem_context = "\n".join([f"- {m.content}" for m in
                                                     relevant_mems]) if relevant_mems else "No relevant memory found."

                            qa_system = "You are a helpful assistant. Answer the question based strictly on the provided memories."
                            prompt_content = f"Memories:\n{mem_context}\n\nQuestion: {question_text}\nAnswer (briefly):"
                            messages = [{"role": "system", "content": qa_system},
                                        {"role": "user", "content": prompt_content}]

                            response_dict = self.llm.chat(messages)

                            if isinstance(response_dict, dict):
                                prediction = response_dict.get("answer") or response_dict.get("content") or str(
                                    response_dict)
                            else:
                                prediction = str(response_dict)

                            f1 = self._calculate_f1(str(prediction), str(gold_answer))
                            all_f1_scores.append(f1)

                # === 3. ã€æ ¸å¿ƒæ–°å¢ã€‘ä¿å­˜æœ¬è½®æå–çš„æ‰€æœ‰è®°å¿† ===
                # åœ¨ reset ä¹‹å‰ï¼ŒæŠŠè®°å¿†åº“é‡Œçš„ä¸œè¥¿æå‡ºæ¥å­˜ç›˜
                final_memories = self.get_all_current_memories()

                record = {
                    "source_id": source_id,
                    "extracted_atom_count": len(final_memories),
                    "memory_atoms": final_memories
                    # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥æŠŠ QA çš„ F1 å­˜ä¸‹æ¥åˆ†æ
                }

                # å†™å…¥ JSONL
                f_out.write(json.dumps(record, ensure_ascii=False) + "\n")
                f_out.flush()  # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­ä¸¢å¤±æ•°æ®

        if all_f1_scores:
            final_score = sum(all_f1_scores) / len(all_f1_scores)
            print(f"\n{'=' * 40}")
            print(f"âœ… [GRPO] è¯„æµ‹å®Œæˆ | Final F1: {final_score:.4f}")
            print(f"ğŸ“‚ è®°å¿†åŸå­å·²ä¿å­˜è‡³: {OUTPUT_MEM_PATH}")
            print(f"{'=' * 40}")
        else:
            print("\nâš ï¸ è·‘å®Œäº†ï¼Œä½†æ²¡æœ‰æœ‰æ•ˆåˆ†æ•°ã€‚")


if __name__ == "__main__":
    evaluator = LocomoStreamEvaluator()
    # å»ºè®®å…ˆè·‘ 5 ä¸ªéªŒè¯ output æ–‡ä»¶å†…å®¹æ˜¯å¦æ­£ç¡®
    evaluator.run(limit=5)